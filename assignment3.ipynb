{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/adams/blob/master/exercises/tut4_NLP_pipeline_teacher.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise NLP pipeline\n",
    "You are already familiar with building predictive models on tabular data. In tabular data, you have a feature matrix `X` and a target vector `Y`. Given these data structures, you can apply learning algorithms like neural networks or random forests to learn the relationship between `X` and `Y`. In this exercise, you are provided with a data set of movie reviews. Your goal is to build a classifier predicting whether a review is positive or negative (this task is called sentiment classification). Hence, you have a prediction problem with a binary target, `Y`, which is nothing new for you. However, in this exercise, what is new for you is that you need to deal with text data instead of tabular data. With text data, you need to process the data to obtain the required feature matrix `X`. This processing of data is what we call the \"NLP pipeline\". \n",
    "\n",
    "In this exercise, you will need to set up an NLP pipeline. You are provided with a data set of movie reviews, where each sample contains a review (just a string cell). To obtain a feature matrix, each sample string cell needs to be transformed into a feature vector $x$. This process is called vectorization. There are multiple possible vectorization procedures. Today, you will implement a bag-of-words model for feature extraction. This feature extraction process involves two steps:\n",
    "1. Vocabulary building\n",
    " * Tokenization: Transforming a review, which is a single string at the beginning, into a vector of strings (tokens).\n",
    " * Cleaning and compressing techniques: Reducing the number of distinct tokens. E.g., correcting the misspelling of words or lower casing the letters prevents the same word from appearing in multiple spelling ways. Additionally, similar words (e.g. different forms of a verb) can be united into a single token. \n",
    " * Building a bag-of-words: a vector whose length corresponds exactly to the number of different tokens. Each token is assigned the position within the vector. \n",
    " \n",
    "2. Feature creation based on term frequency: Each review gets transformed into a feature vector $x$. The length of the feature vector corresponds to the length of the bag-of-words vector, created in step 1. An element $x_{j}$ of the feature vector is calculated by a frequency measure, measuring how frequently token $j$ from the bag-of-words vector occurs in the review. \n",
    "\n",
    "The first code cells provide the required packages and load the review data set, which you will use for the exercise. In the exercise, you will build the most simple NLP pipeline, which means that you go through steps 1 and 2 of the NLP pipeline, but you skip the \"cleaning and compressing\" part of step 1. This simple NLP pipeline provides you with a feature matrix `X` (possibly not ideal). You will use this feature matrix to build and evaluate a predictive model.\n",
    "\n",
    "In the tutorial, we will extend your NLP pipeline by including the cleaning and compressing techniques (according techniques are also covered in detail in the demo notebook `nlp_foundations.ipynb`). That will lead to another feature matrix, `X`. Then, we will build another predictive model on this new feature matrix `X` and compare the performance to the model built by the simplified NLP pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required packages\n",
    "import pandas as pd\n",
    "import nltk\n",
    "# nltk.download('punkt') If needed\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from bs4 import BeautifulSoup ## handles html\n",
    "import re ## provides regular expressions functionality\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# Remeber to adjust the path so that it matches your environment\n",
    "df = pd.read_csv(\"IMDB-50K-Movie-Review.csv\", sep=\",\", encoding=\"ISO-8859-1\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  review sentiment\n",
      "0      One of the other reviewers has mentioned that ...  positive\n",
      "1      A wonderful little production. <br /><br />The...  positive\n",
      "2      I thought this was a wonderful way to spend ti...  positive\n",
      "3      Basically there's a family where a little boy ...  negative\n",
      "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "...                                                  ...       ...\n",
      "49995  I thought this movie did a down right good job...  positive\n",
      "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
      "49997  I am a Catholic taught in parochial elementary...  negative\n",
      "49998  I'm going to have to disagree with the previou...  negative\n",
      "49999  No one expects the Star Trek movies to be high...  negative\n",
      "\n",
      "[50000 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get to know the data\n",
    "print(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    5028\n",
       "negative    4973\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only use the first 10000 observations to reduce run time.\n",
    "df = df.loc[0:10000,:]\n",
    "\n",
    "df.reset_index(inplace=True, drop=True)  # dropping the index prohibits a reidentification of the cases in the original data frame\n",
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  One of the other reviewers has mentioned that ...          1\n",
       "1  A wonderful little production. <br /><br />The...          1\n",
       "2  I thought this was a wonderful way to spend ti...          1\n",
       "3  Basically there's a family where a little boy ...          0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...          1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map label\n",
    "df['sentiment'] = df['sentiment'].map({'positive' : 1, 'negative': 0})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise (simple NLP pipeline):\n",
    "You need to transform the text data, contained in the column `df[\"review\"]`, such that it is suitable as a feature matrix `X`, which you need for predictive model building. This means in detail: \n",
    "\n",
    "a) Create a list \"reviews_tokenized\", where each element corresponds to a string vector, representing a review. Use NLTK's `word_tokenize()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'world']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize('Hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        [One, of, the, other, reviewers, has, mentione...\n",
      "1        [A, wonderful, little, production, ., <, br, /...\n",
      "2        [I, thought, this, was, a, wonderful, way, to,...\n",
      "3        [Basically, there, 's, a, family, where, a, li...\n",
      "4        [Petter, Mattei, 's, ``, Love, in, the, Time, ...\n",
      "                               ...                        \n",
      "9996     [Give, me, a, break, ., How, can, anyone, say,...\n",
      "9997     [This, movie, is, a, bad, movie, ., But, after...\n",
      "9998     [This, is, a, movie, that, was, probably, made...\n",
      "9999     [Smashing, film, about, film-making, ., Shows,...\n",
      "10000    [``, While, sporadically, engrossing, (, inclu...\n",
      "Length: 10001, dtype: object\n"
     ]
    }
   ],
   "source": [
    "reviews_tokenized = df.apply(lambda row: nltk.word_tokenize(row['review']), axis=1)\n",
    "print(reviews_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Split the review data (`reviews_tokenized`) as well as the target `df['sentiment']` in training and test sets. Use 80% of the data for training. Use sklearn's `train_test_split()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(reviews_tokenized, df['sentiment'], test_size = 0.2, random_state = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Now, we need to set up a vocabulary for all tokens and apply this vocabulary to obtain feature vectors $x$. We do this using sklearn's `TfidfVectorizer`. We provide the code to set up the vectorizer below. You need to apply the vectorizer to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_fun(doc):\n",
    "    return doc       \n",
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer = 'word',\n",
    "    tokenizer = dummy_fun,\n",
    "    preprocessor = dummy_fun,\n",
    "    token_pattern = None)\n",
    "\n",
    "## Set up the dictionary and calculate the document frequency of each token on the training set.\n",
    "## Then generate the features on the training set, using the document frequency table.\n",
    "reviews_tr = vectorizer.fit_transform(X_train)\n",
    "\n",
    "## Apply the document frequency table one the test set, to generate feature vectors.\n",
    "reviews_ts = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TfidfVectorizer` did multiple steps at once. To better understand how it works, you should examine the results step by step.\n",
    "\n",
    "d) Examine the vocabulary it created: How many tokens does it include? Which tokens are included? Would it maybe be better to leave some of these tokens out to reduce the dimension of the vocabulary and the derived feature matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary contains 74267 tokens.\n",
      "Now let us look at some examples of these tokens.\n",
      "['\\x10own', '!', '#', '$', '%', '&', \"'\", \"''\", \"''Headin\", \"''Scarface\", \"''Wallace\", \"'*name\", \"'.\", \"'007\", \"'00s\", \"'01\", \"'02\", \"'03\", \"'04\", \"'05\", \"'06\", \"'07\", \"'10\", \"'12\", \"'15\", \"'1st\", \"'20th\", \"'28\", \"'30\", \"'30s\", \"'30s-'40s\", \"'30s-Ray\", \"'30s/'40s\", \"'40\", \"'40s\", \"'42\", \"'43\", \"'45\", \"'50\", \"'50s\", \"'50s/early\", \"'51\", \"'54-'55\", \"'59\", \"'60\", \"'60s\", \"'60s-early\", \"'61\", \"'63\", \"'66\"]\n"
     ]
    }
   ],
   "source": [
    "vocab = vectorizer.get_feature_names()\n",
    "print('The vocabulary contains ' + str(len(vocab)) + ' tokens.')\n",
    "print('Now let us look at some examples of these tokens.')\n",
    "print(vocab[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Let's recap how feature vectors are generated from this vocabulary. The basic idea of bag-of-words based feature extraction is to generate for each token in the vocabulary a column in the feature matrix `X`. For an observation $i$ (corresponding to a single review), the entry $X_{i,j}$ of the feature matrix would be 1 if the review contains the token of column $j$ and 0 otherwise. There are some variations to this approach. The Tfidf approach (*term frequency-inverse document frequency*), which we apply in this exercise. This encodes $X_{i,j}$ not as 1 if the review contains token $j$ but as the occurrence frequency of token $j$ in the review divided by the occurrence frequency of token $j$ in the whole document (all reviews of the data set combined). Have a look at the matrix, which the `TfidfVectorizer` created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.        0.        0.        0.        0.        0.        0.\n",
      "  0.0742515 0.        0.        0.        0.        0.        0.\n",
      "  0.        0.        0.        0.        0.        0.        0.\n",
      "  0.        0.        0.        0.        0.        0.        0.\n",
      "  0.        0.        0.        0.        0.        0.        0.\n",
      "  0.        0.        0.        0.        0.        0.        0.\n",
      "  0.        0.        0.        0.        0.        0.        0.\n",
      "  0.        0.        0.        0.        0.        0.        0.\n",
      "  0.        0.        0.        0.        0.        0.        0.\n",
      "  0.        0.        0.        0.        0.        0.        0.\n",
      "  0.        0.        0.        0.        0.        0.        0.\n",
      "  0.        0.        0.        0.        0.        0.        0.\n",
      "  0.        0.        0.        0.        0.        0.        0.\n",
      "  0.        0.        0.        0.        0.        0.        0.\n",
      "  0.        0.       ]]\n",
      "['One', 'of', 'the', 'other', 'reviewers', 'has', 'mentioned', 'that', 'after', 'watching', 'just', '1', 'Oz', 'episode', 'you', \"'ll\", 'be', 'hooked', '.', 'They', 'are', 'right', ',', 'as', 'this', 'is', 'exactly', 'what', 'happened', 'with', 'me.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'The', 'first', 'thing', 'that', 'struck', 'me', 'about', 'Oz', 'was', 'its', 'brutality', 'and', 'unflinching', 'scenes', 'of', 'violence', ',', 'which', 'set', 'in', 'right', 'from', 'the', 'word', 'GO', '.', 'Trust', 'me', ',', 'this', 'is', 'not', 'a', 'show', 'for', 'the', 'faint', 'hearted', 'or', 'timid', '.', 'This', 'show', 'pulls', 'no', 'punches', 'with', 'regards', 'to', 'drugs', ',', 'sex', 'or', 'violence', '.', 'Its', 'is', 'hardcore', ',', 'in', 'the', 'classic', 'use', 'of', 'the', 'word.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'It', 'is', 'called', 'OZ', 'as', 'that', 'is', 'the', 'nickname', 'given', 'to', 'the', 'Oswald', 'Maximum', 'Security', 'State', 'Penitentary', '.', 'It', 'focuses', 'mainly', 'on', 'Emerald', 'City', ',', 'an', 'experimental', 'section', 'of', 'the', 'prison', 'where', 'all', 'the', 'cells', 'have', 'glass', 'fronts', 'and', 'face', 'inwards', ',', 'so', 'privacy', 'is', 'not', 'high', 'on', 'the', 'agenda', '.', 'Em', 'City', 'is', 'home', 'to', 'many..Aryans', ',', 'Muslims', ',', 'gangstas', ',', 'Latinos', ',', 'Christians', ',', 'Italians', ',', 'Irish', 'and', 'more', '...', '.so', 'scuffles', ',', 'death', 'stares', ',', 'dodgy', 'dealings', 'and', 'shady', 'agreements', 'are', 'never', 'far', 'away.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'I', 'would', 'say', 'the', 'main', 'appeal', 'of', 'the', 'show', 'is', 'due', 'to', 'the', 'fact', 'that', 'it', 'goes', 'where', 'other', 'shows', 'would', \"n't\", 'dare', '.', 'Forget', 'pretty', 'pictures', 'painted', 'for', 'mainstream', 'audiences', ',', 'forget', 'charm', ',', 'forget', 'romance', '...', 'OZ', 'does', \"n't\", 'mess', 'around', '.', 'The', 'first', 'episode', 'I', 'ever', 'saw', 'struck', 'me', 'as', 'so', 'nasty', 'it', 'was', 'surreal', ',', 'I', 'could', \"n't\", 'say', 'I', 'was', 'ready', 'for', 'it', ',', 'but', 'as', 'I', 'watched', 'more', ',', 'I', 'developed', 'a', 'taste', 'for', 'Oz', ',', 'and', 'got', 'accustomed', 'to', 'the', 'high', 'levels', 'of', 'graphic', 'violence', '.', 'Not', 'just', 'violence', ',', 'but', 'injustice', '(', 'crooked', 'guards', 'who', \"'ll\", 'be', 'sold', 'out', 'for', 'a', 'nickel', ',', 'inmates', 'who', \"'ll\", 'kill', 'on', 'order', 'and', 'get', 'away', 'with', 'it', ',', 'well', 'mannered', ',', 'middle', 'class', 'inmates', 'being', 'turned', 'into', 'prison', 'bitches', 'due', 'to', 'their', 'lack', 'of', 'street', 'skills', 'or', 'prison', 'experience', ')', 'Watching', 'Oz', ',', 'you', 'may', 'become', 'comfortable', 'with', 'what', 'is', 'uncomfortable', 'viewing', '...', '.thats', 'if', 'you', 'can', 'get', 'in', 'touch', 'with', 'your', 'darker', 'side', '.']\n"
     ]
    }
   ],
   "source": [
    "## print the first 100 feature entries for the first review\n",
    "print(reviews_tr[0,0:100].todense())\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Fit a ridge regression classifier (`RidgeClassifier`) and evaluate the accuracy of the predictions on the training and test sets. We provide the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.996125\n",
      "0.8690654672663668\n"
     ]
    }
   ],
   "source": [
    "classifier = RidgeClassifier(random_state=42, alpha=0.8)\n",
    "classifier.fit(reviews_tr, y_train)\n",
    "pred_test = classifier.predict(reviews_ts)\n",
    "pred_train = classifier.predict(reviews_tr)\n",
    "print(metrics.accuracy_score(y_train, pred_train))\n",
    "print(metrics.accuracy_score(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial exercise\n",
    "In the previous exercise, we took little care about the cleaning and compressing part of the NLP pipeline. As a consequence, we obtained a dictionary with a lot of tokens which are most likely not so informative. The high number of tokens in the dictionary resulted in a very high dimension of the feature matrix `X`. In this exercise, we will add the cleaning and compressing part to the NLP pipeline. We hope to create a feature matrix of lower dimension, which yields more accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/hawk/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/hawk/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/hawk/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/hawk/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## download pre-learned NLP tools\n",
    "nltk.download('stopwords') ## to identify stopwords \n",
    "nltk.download('averaged_perceptron_tagger') ## for part-of-speech tagging (used for lemmatization)\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Lemmatize with POS Tag (Parts of Speech tagging)\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character for lemmatization\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "## function to clean text data\n",
    "def clean_reviews(df):\n",
    "    \"\"\" Standard NLP pre-processing chain including removal of html tags, non-alphanumeric characters, and stopwords.\n",
    "        Words are subject to lemmatization using their POS tags, which are determind using WordNet. \n",
    "    \"\"\"\n",
    "    reviews = []\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    print('*' * 40)\n",
    "    print('Cleaning {} movie reviews.'.format(df.shape[0]))\n",
    "    counter = 0\n",
    "    for review in df:\n",
    "        \n",
    "        # remove html content\n",
    "        review_text = BeautifulSoup(review).get_text()\n",
    "        \n",
    "        # remove non-alphabetic characters\n",
    "        review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    \n",
    "        # tokenize the sentences with all capital letters transformed to lower case\n",
    "        words = word_tokenize(review_text.lower())\n",
    "  \n",
    "        # filter stopwords\n",
    "        words = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "        \n",
    "        # lemmatize each word to its lemma\n",
    "        lemma_words =[lemmatizer.lemmatize(i, get_wordnet_pos(i)) for i in words]\n",
    "    \n",
    "        reviews.append(lemma_words)\n",
    "              \n",
    "        if (counter > 0 and counter % 500 == 0):\n",
    "            print('Processed {} reviews'.format(counter))\n",
    "            \n",
    "        counter += 1\n",
    "        \n",
    "    print('DONE')\n",
    "    print('*' * 40)\n",
    "\n",
    "    return(reviews) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "Cleaning 10001 movie reviews.\n",
      "Processed 500 reviews\n",
      "Processed 1000 reviews\n",
      "Processed 1500 reviews\n",
      "Processed 2000 reviews\n",
      "Processed 2500 reviews\n",
      "Processed 3000 reviews\n",
      "Processed 3500 reviews\n",
      "Processed 4000 reviews\n",
      "Processed 4500 reviews\n",
      "Processed 5000 reviews\n",
      "Processed 5500 reviews\n",
      "Processed 6000 reviews\n",
      "Processed 6500 reviews\n",
      "Processed 7000 reviews\n",
      "Processed 7500 reviews\n",
      "Processed 8000 reviews\n",
      "Processed 8500 reviews\n",
      "Processed 9000 reviews\n",
      "Processed 9500 reviews\n",
      "Processed 10000 reviews\n",
      "DONE\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "#* Do the cleaning\n",
    "# CAUTION: takes around 20 minutes \n",
    "reviews_clean = clean_reviews(df.review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save cleaned data\n",
    "open_file = open('reviews_clean.pkl', \"wb\")\n",
    "pickle.dump(reviews_clean, open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load cleaned data\n",
    "open_file = open('reviews_clean.pkl', \"rb\")\n",
    "reviews_clean = pickle.load(open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v\n",
      "n\n"
     ]
    }
   ],
   "source": [
    "## While the text gets cleaned, we have a look at the part-of-speech-tagging and lemmatization part of the cleaning function\n",
    "## part-of-speech tagging identifies the word category (whether a word is a verb, noun, adjective, or adverb)\n",
    "print(get_wordnet_pos('running'))\n",
    "print(get_wordnet_pos('runner'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "runner\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "## the word categorie determines how to lemmatize the word\n",
    "lemmatizer_example = WordNetLemmatizer()\n",
    "print(lemmatizer_example.lemmatize('running',get_wordnet_pos('running')))\n",
    "print(lemmatizer_example.lemmatize('runner',get_wordnet_pos('runner')))\n",
    "print(lemmatizer_example.lemmatize('run',get_wordnet_pos('run')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split reviews in training and test set\n",
    "Xclean_train, Xclean_test, y_train, y_test = train_test_split(reviews_clean, df['sentiment'], test_size = 0.2, random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply tfidf feature extraction\n",
    "vectorizer_clean = TfidfVectorizer(\n",
    "    analyzer = 'word',\n",
    "    tokenizer = dummy_fun,\n",
    "    preprocessor = dummy_fun,\n",
    "    token_pattern = None)\n",
    "\n",
    "## apply tfidf to training set and create vocabulary\n",
    "reviews_clean_tr = vectorizer_clean.fit_transform(Xclean_train)\n",
    "\n",
    "## Apply the document frequency table one the test set, to generate feature vectors.\n",
    "reviews_clean_ts = vectorizer_clean.transform(Xclean_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary contains 38070 tokens.\n",
      "Now let us look at some examples of these tokens.\n",
      "['aa', 'aaa', 'aaaaahhhh', 'aaaarrgh', 'aaah', 'aaall', 'aaargh', 'aaaugh', 'aag', 'aah', 'aaip', 'aaliyah', 'aames', 'aamir', 'aamto', 'aankhen', 'aap', 'aardman', 'aaron', 'aarp', 'aashok', 'aatish', 'aavjo', 'aawip', 'ab', 'abandon', 'abandonment', 'abash', 'abba', 'abbey', 'abbie', 'abbot', 'abbott', 'abbreviate', 'abc', 'abdalla', 'abdic', 'abdomen', 'abduct', 'abductee', 'abduction', 'abductor', 'abducts', 'abdul', 'abe', 'abedded', 'abel', 'abemethie', 'abernathie', 'abernethie']\n"
     ]
    }
   ],
   "source": [
    "## analyze vocabulary\n",
    "vocab_clean = vectorizer_clean.get_feature_names()\n",
    "print('The vocabulary contains ' + str(len(vocab_clean)) + ' tokens.')\n",
    "print('Now let us look at some examples of these tokens.')\n",
    "# print(vocab_clean[10000:10050])\n",
    "print(vocab_clean[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.993125\n",
      "0.8690654672663668\n"
     ]
    }
   ],
   "source": [
    "## apply and evaluate classifier on clean text data\n",
    "classifier_clean = RidgeClassifier(random_state=42, alpha=0.8)\n",
    "classifier_clean.fit(reviews_clean_tr, y_train)\n",
    "pred_test = classifier_clean.predict(reviews_clean_ts)\n",
    "pred_train = classifier_clean.predict(reviews_clean_tr)\n",
    "print(metrics.accuracy_score(y_train, pred_train))\n",
    "print(metrics.accuracy_score(y_test, pred_test))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "071737a5efb5187f1b8a7f5eacd9bb694a30cbbaa4393dd0a3bebb490d9d36dd"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
